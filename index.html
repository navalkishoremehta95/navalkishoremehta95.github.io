<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="Files/jemdoc.css" type="text/css" />

<link rel="shortcut icon" href="./Files/logo.ico">
<title>Naval Kishore Metha's Personal Website - Enjoy your visit</title>
</head>
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./"><img src="./Files/Naval.png" alt="" height="220px" /></a>&nbsp;</td>
<td align="left"><p><font size="5"> <b> Naval Kishore Mehta</font>
   </b> </font></a><br />

  <hr>

<p><font size="4"> <b> Researcher  </b> 

<p/>
<a href="https://www.hiroshima-u.ac.jp/en/adse">Hiroshima University <br />
<p/>

<font size="3"> 
Email: <a href="mailto:naval.ceeri18a@acsir.res.in">naval.ceeri18a@acsir.res.in</a><br />
 

<p/>
<font size="3">

<a href="https://scholar.google.com/citations?user=iMKNqZMAAAAJ&hl=en"> <img border="0" src="./Files/google.png" width="30" height="30">  </a> &nbsp;&nbsp;
<a href="https://www.researchgate.net/profile/Naval-Kishore-Mehta-2"> <img border="0" src="./Files/rg.png" width="30" height="30"> </a> &nbsp;&nbsp;
<a href="https://dblp.org/pid/330/6163.html"> <img border="0" src="./Files/dblp.png" width="30" height="30"> </a> &nbsp;&nbsp;
<a href="https://www.linkedin.com/in/naval-kishore-m-33141378/"> <img border="0" src="./Files/linkedin.png" width="30" height="30"> </a> &nbsp;&nbsp;
<!-- <a href="https://twitter.com/CaoZehong/"> <img border="0" src="./Files/twitter.png" width="30" height="30"> </a> &nbsp;&nbsp; -->
<a href="https://github.com/navalkishoremehta95">  <img border="0" src="./Files/git.png" width="30" height="30"> </a> &nbsp;&nbsp;

</p>


</td></tr></table>

<class="staffshortcut"> 

&nbsp;&nbsp;

<font size="3">
<A HREF="#Introduction">Introduction</A> | 
<!-- <A HREF="#Recent News">Recent News</A> |  -->
<!-- <A HREF="#Research Interests">Research Areas</A> |  -->
<A HREF="#Research">Research</A> | 
<A HREF="#Services">Volunteer Services</A> 
<!--    |    -->
<!-- <A HREF="#Awards">Awards</A>   -->
<br />
<br />


<!-- Short Summary-->
<A NAME="Introduction"><h2>Introduction</h2></A>
<ul>

<li> 

I am currently a Young Researcher at Hiroshima University, Japan, and have recently submitted my Ph.D. as part of the Integrated Dual-Degree (M.Tech + Ph.D.) programme at <a href="https://acsir.res.in/"> AcSIR</a>, <a href="https://www.csircmc.res.in/ceeri"> | CSIR-CEERI</a>, where I worked in the <a href="https://visionai-group.github.io/">Advanced Information Technologies Group (AITG)</a> under the guidance of <a href="https://sites.google.com/site/sanjaycsirceeri/">Dr. Sanjay Singh</a> and <a href="https://www.ceeri.res.in/profiles/ravi-saini/">Dr. Ravi Saini</a>. My research focuses on humanâ€“computer interaction and video understanding, where I integrate deep learning with industrial applications in multimodal activity monitoring, predictive safety systems, and human-centric AI for collaborative environments.

</li> 

</p> 

<li> 

Before joining CSIR-CEERI, I worked as a Research Intern at <a href="https://www.iitb.ac.in/">IIT Bombay</a> in the Physics and MEMS departments under the mentorship of <a href="https://www.linkedin.com/in/shiva-prasad-343aa08?originalSubdomain=in">Prof. Shiva Prasad</a> and <a href="https://www.iitb.ac.in/mems/en/prof-n-venkataramani">Prof. N. Venkataramani</a>. I hold a Master's degree in Electronics Science from <a href="https://kuk.ac.in/department_of_electronic_science/">Kurukshetra University</a> and a Bachelor's degree in Electronics (Hons.) from the <a href="https://www.du.ac.in/">University of Delhi</a>.
 

</ul>

 

<!-- Research Areas -->
<!-- <A NAME="Research Areas"><h2>Research Areas</h2></A> -->

<!-- <div align="center"> <a href="./"><img src="./Files/my research territory.png" alt="" height="310px" /> </a> </div>  -->

 
 
<!--Publications -->
<A NAME="Research"><h2>Research</h2></A>

<ul>

<ul>
<!-- <li> <b>A full list of my publications </b> can be found from my <a href= "https://scholar.google.com/citations?user=iMKNqZMAAAAJ&hl=en>Google Scholar</a> profile. </li> -->
<!-- 
<li> </p> My significant research outcomes have been published by top-tier journal/conference venues (<a href="https://www.core.edu.au/conference-portal">CORE A*</a> &
<a href="https://www.scimagojr.com/journalrank.php"> JCR Q1</a>), such as
<i>IEEE TPAMI, TNNLS, TFS, TKDE, TMI, TNSRE, JBHI, AAMAS, AAAI</i> ... </li> </p>
</ul> -->
<!-- Project Section -->


   
<table>
  <tr bgcolor="#e6f7ff">
    <!-- Right: GIF/Image -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="image-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <img id="static_image" src="./Files/hri_25.png" width="180" height="130" style="position: absolute;">
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var gif = document.getElementById('gif_image');
          var staticImage = document.getElementById('google');
          
          // Check if the GIF loads successfully
          gif.onerror = function () {
            gif.style.display = 'none';
            staticImage.style.display = 'block';
          };

          gif.onload = function () {
            gif.style.opacity = '1';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://arxiv.org/abs/2501.05936">A Multimodal Dataset for Enhancing Industrial Task Monitoring
and Engagement Prediction</a>
      </papertitle>
      <br>
       <strong>Naval Kishore Mehta</strong>, Arvind, Himanshu Kumar, Abeer Banerjee, Sumeet Saurav, Sanjay Singh<br>
      <em>IEEE/ACM International Conference on Human-Robot Interaction, 2025 (Oral) (CORE-A)</em><br>  
      <a href="https://dl.acm.org/doi/10.5555/3721488.3721619">Paper</a> |
      <a href="https://visionai.ceeri.res.in/dataweb/miam/miam1.html">Dataset</a> |
       <a href="https://github.com/navalkishoremehta95/MIAM/">Code</a>

      <p></p>
      <p>
   This project tackles the challenge of detecting operator actions, engagement, and object interactions in dynamic industrial workflows. We introduce a multimodal dataset featuring RGB, depth, and IMU data from 20 sessions (176 minutes of untrimmed video) annotated for action localization, object interaction, and engagement prediction. Also, we propose a multimodal network that fuses RGB, IMU, and skeleton data to improve engagement prediction accuracy, advancing research in human-robot collaboration and operator monitoring.      </p>
    </td>
  </tr>
</table>
   


   <table>
  <!-- <tr bgcolor="#f5f5dc"> -->
  <tr style="background-color: transparent;">

    <!-- Right: Video -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="video-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <!-- Static Image as Fallback -->
        <img id="static_image" src="./Files/icvng24.png" width="180" height="130" style="position: absolute;">
        
      </div>
     </td>

    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://ieeexplore.ieee.org/abstract/document/10794202/">Gaze Estimation via Synthetic Event-Driven Neural Networks</a>
      </papertitle>
      <br>
      Himanshu Kumar, <strong>Naval Kishore Mehta</strong>,  Sumeet Saurav, Sanjay Singh<br>
      <em> Image and Vision Computing New Zealand (IVCNZ), 2024 (Oral)</em><br>  

      <a href="https://ieeexplore.ieee.org/abstract/document/10794202/">Paper</a>   
       <p></p>
      <p>
      This project focuses on improving gaze estimation by bridging the gap between synthetic and real event data. It introduces a synthetic gaze event dataset capturing saccadic dynamics, proposes a novel Dual ResUNet architecture for accurate gaze prediction, and develops an end-to-end algorithm using the v2e simulator and event encoding to enable precise estimation in fully synthetic environments.
      </p>
    </td>
  </tr>
</table>



<table>
  <tr bgcolor="#f5f5dc">
  
    <!-- Right: Video -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="video-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <!-- Static Image as Fallback -->
        <img id="static_image" src="./Files/df_sampler.png" width="180" height="130" style="position: absolute;">
        
      </div>
     </td>

    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://ieeexplore.ieee.org/document/10711155">DF Sampler: A Self-Supervised Method for Adaptive Keyframe Sampling</a>
      </papertitle>
      <br>
      <strong>Naval Kishore Mehta</strong>, Shyam Sunder Prasad, Sumeet Saurav, Sanjay Singh<br>
      <em> International Conference on Emerging Technologies and Factory Automation (ETFA), 2024 (Oral)</em><br>  
      <a href="https://ieeexplore.ieee.org/document/10711155">Paper</a>   
      <!-- <a href="https://github.com/navalkishoremehta95/IAR-NET">Code</a> -->
      <p></p>
      <p>
        We introduce Dynamic Frame Sampler (DF Sampler), a novel adaptive keyframe sampling method for variable-duration action sequences in complex industrial settings. Using a self-supervised approach, DF Sampler selects motion-relevant keyframes to enhance HAR system efficiency. Its effectiveness is validated on the HRI30 dataset.
      </p>
    </td>
  </tr>
</table>

 
<!-- 
      Annual Conference of the IEEE Industrial Electronics Society (IECON), 2024
      
      
        To address the challenge of real-time anomaly detection, which is critical for maintaining productivity and efficiency, we introduce PROACT. This novel framework predicts operator actions and identifies anomalies by constructing a reference graph from action sequences. By anticipating behavior and detecting deviations early, PROACT ensures enhanced safety and operational efficiency in dynamic environments.
     
 -->


<table>
  <tr bgcolor="#f5f5dc">
    <!-- Right: Video -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="video-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <!-- Static Image as Fallback -->
        <!-- <img id="static_image" src="./Files/iar_net.png" width="160" height="140" style="position: absolute;"> -->
        
        <!-- MP4 Video -->
        <video id="mp4_video" width="180" height="140" style="display: block;" autoplay muted loop>
          <source src="./Files/iar_net.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var video = document.getElementById('mp4_video');
          var staticImage = document.getElementById('static_image');
          
          // Check if the video loads successfully
          video.onerror = function () {
            console.log("Video failed to load.");
            video.style.display = 'none';
            staticImage.style.display = 'block';
          };

          video.onloadstart = function () {
            console.log("Video loaded successfully.");
            video.style.display = 'block';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://ieeexplore.ieee.org/document/10517873">IAR-Net: A Humanâ€“Object Context Guided Action Recognition Network for Industrial Environment Monitoring</a>
      </papertitle>
      <br>
      <strong>Naval Kishore Mehta</strong>, Shyam Sunder Prasad, Sumeet Saurav, Ravi Saini, Sanjay Singh<br>
      <em>IEEE Transactions on Instrumentation and Measurement, 2024</em><br>  
      <a href="https://ieeexplore.ieee.org/document/10517873">Paper</a> | 
      <a href="https://github.com/navalkishoremehta95/IAR-NET">Code</a>
      <p></p>
      <p>
        This project introduces IAR-Net, a novel neural network that integrates human and object context for industrial human activity recognition. It is complemented by the specialized LAMIS database, a first-of-its-kind dataset featuring 17 industrial action categories. Also, the work proposes Adaptive Frame Sampling (AFS), a novel frame selection technique evaluated against existing methods. AFS-IAR-Net achieves state-of-the-art performance on the HRI30 benchmark and LAMIS database, advancing SOP monitoring and industrial ergonomic analysis.
      </p>
    </td>
  </tr>
</table>


<table>
  <tr bgcolor="#e6f7ff">
    <!-- Right: GIF/Image -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="image-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <img id="google" src="./Files/ijcnn24.png" width="180" height="140" style="position: absolute;">
        <!-- <img id="gif_image" src="./Files/animated_image.gif" width="160" height="140" style="opacity: 0; position: relative;"> -->
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var gif = document.getElementById('gif_image');
          var staticImage = document.getElementById('google');
          
          // Check if the GIF loads successfully
          gif.onerror = function () {
            gif.style.display = 'none';
            staticImage.style.display = 'block';
          };

          gif.onload = function () {
            gif.style.opacity = '1';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://ieeexplore.ieee.org/abstract/document/10650655">Generalized Gaze-Vector Estimation in Low-light with Encoded Event-driven Neural Network</a>
      </papertitle>
      <br>
      Abeer Banerjee, <strong>Naval Kishore Mehta</strong>, Shyam S Prasad, Himanshu Kumar, Sumeet Saurav, Sanjay Singh<br>
      <em>International Joint Conference on Neural Networks (IJCNN), 2024 (Oral) CORE-B</em><br>  
      <a href="https://ieeexplore.ieee.org/abstract/document/10650655">Paper</a>
      <p></p>
      <p>
        This project focuses on overcoming the challenges of gaze vector prediction in extremely low-light environments. By introducing a unique temporal event-encoding approach and designing a tailored neural network, it ensures precise spatial localization and consistent accuracy in gaze vector estimation.
      </p>
    </td>
  </tr>
</table>


<table>
  <tr bgcolor="#f5f5dc">
    <!-- Right: Video -->
    <td style="padding:10px; width:25%; vertical-align:middle;">
      <div class="video-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <!-- WebM Video -->
        <video id="video_demo" width="180" height="140" autoplay muted loop controls>
          <source src="./Files/Vton.webm" type="video/webm">
          Your browser does not support the video tag.
        </video>
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var video = document.getElementById('video_demo');

          // Log video load status
          video.onerror = function () {
            console.log("Video failed to load.");
          };

          video.onloadstart = function () {
            console.log("Video loading...");
          };
        };
      </script>
    </td>
    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>

        Virtual-Try-On
      </papertitle>
      <br>  
      <strong>Naval Kishore Mehta</strong>,  Shyam S Prasad, Deepak Kumar</a> <br>
      <!-- <em> <a href="https://ssp.jst.go.jp/en/">Sakura Science Exchange Program Fellow, 2019</a> </em><br> -->
      <!-- <em> <a href="http://www.ildp.hiroshima-u.ac.jp/">International Linkage Degree Program Fellowship (Hiroshima University, Japan), 2019</a> </em><br> -->
      <p></p>
      <p>
      	This project developed a virtual try-on system, leveraging diffusion models and appearance flow techniques to address high-resolution image challenges such as misalignment and occlusions. By integrating a precise warping network, the system significantly improved the realism of digital apparel fittings.              </p>
    </td>
  </tr>
</table>




<table>
  <tr bgcolor="#f5f5dc">
    <!-- Right: Video -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="video-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <!-- Static Image as Fallback -->
        <img id="static_image" src="./Files/js-spoof.png" width="180" height="140" style="position: absolute;">
        
      </div>
     </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231223007336">JS-SpoofNet: A Jointly Supervised Parallel Branched Neural Network for Spoof Detection</a>
      </papertitle>
      <br>
      Shyam Sunder Prasad, <strong>Naval Kishore Mehta</strong>, Abeer Banerjee, Sumeet Saurav, Sanjay Singh<br>
      <em>Neurocomputing Journal, Elsevier, 2023</em><br>  
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231223007336">Paper</a> | 
      <a href="https://github.com/navalkishoremehta95/JS-SpoofNet">Code</a>
      <p></p>
      <p>
       This project introduces a jointly supervised parallel branched neural network designed for robust video-based spoof detection in real-world scenarios. The auxiliary branch enhances the primary network by estimating depth through intermediate feature fusion, improving detection accuracy. It is complemented by the CSDiNE dataset, a large-scale, in-house video dataset tailored for spoof detection under varied illumination and background conditions. The proposed network achieves state-of-the-art performance with low average classification error rate (ACER) of 0.94% on the CSDiNE dataset, demonstrating computational efficiency and robustness for practical applications.  
      </p>
    </td>
  </tr>
</table>




<table>
  <tr bgcolor="#f5f5dc">
    <!-- Right: GIF/Image -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="image-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <img id="google" src="./Files/hsp2fdns.png" width="160" height="140" style="position: absolute;">
        <!-- <img id="gif_image" src="./Files/animated_image.gif" width="160" height="140" style="opacity: 0; position: relative;"> -->
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var gif = document.getElementById('gif_image');
          var staticImage = document.getElementById('google');
          
          // Check if the GIF loads successfully
          gif.onerror = function () {
            gif.style.display = 'none';
            staticImage.style.display = 'block';
          };

          gif.onload = function () {
            gif.style.opacity = '1';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://dl.acm.org/doi/abs/10.1145/3627631.3627650">Hybrid SNN-based Privacy-Preserving Fall Detection using Neuromorphic Sensors
</a>
      </papertitle>
      <br>
      Shyam Sunder Prasad, <strong>Naval Kishore Mehta</strong>, Himanshu Kumar, Abeer Banerjee, Sumeet Saurav, Sanjay Singh<br>
      <em> Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP), 2023</em><br>  
      <a href="https://dl.acm.org/doi/abs/10.1145/3627631.3627650">Paper</a>
      <p></p>
      <p>
        This paper presents a fall detection framework utilizing Spiking Neural Networks (SNN) to replicate neural activity, achieving an accuracy of 94.59%. Furthermore, it explores a hybrid approach combining 3D-CNN & SNN (NeuCube), which significantly improves detection accuracy to 97.84% on the recorded dataset.    </p>
    </td>
  </tr>
</table>



<table>
  <tr bgcolor="#e6f7ff">
    <!-- Right: GIF/Image -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="image-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <img id="google" src="./Files/applied_int22.png" width="160" height="140" style="position: absolute;">
        <!-- <img id="gif_image" src="./Files/animated_image.gif" width="160" height="140" style="opacity: 0; position: relative;"> -->
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var gif = document.getElementById('gif_image');
          var staticImage = document.getElementById('google');
          
          // Check if the GIF loads successfully
          gif.onerror = function () {
            gif.style.display = 'none';
            staticImage.style.display = 'block';
          };

          gif.onload = function () {
            gif.style.opacity = '1';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://link.springer.com/article/10.1007/s10489-022-03200-4">Three-dimensional DenseNet self-attention neural network for automatic detection of studentâ€™s engagement
</a>
      </papertitle>
      <br>
      <strong>Naval Kishore Mehta</strong>, Shyam Sunder Prasad, Sumeet Saurav, Ravi Saini, Sanjay Singh<br>
      <em> Applied Intelligence Journal, Springer,  2022</em><br>  
      <a href="https://link.springer.com/article/10.1007/s10489-022-03200-4">Paper</a>  
      <p></p>
      <p>
        This paper presents DenseAttNet, a self-attention DenseNet model achieving state-of-the-art performance in evaluating student engagement in virtual and traditional learning settings. On the DAiSEE dataset, it records engagement and boredom classification accuracies of 63.59% and 54.27%, respectively, and excels in multi-label tasks with accuracies up to 95.85%. Regression experiments further validate its effectiveness, achieving an MSE of 0.0347 on DAiSEE and 0.0877 on EmotiW-EP, establishing DenseAttNet as a robust solution for analyzing emotional states in e-learning.
     </p>
    </td>
  </tr>
</table>




<table>
  <tr bgcolor="#f5f5dc">
    <!-- Right: GIF/Image -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="image-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <img id="google" src="./Files/ihci22.png" width="160" height="140" style="position: absolute;">
        <!-- <img id="gif_image" src="./Files/animated_image.gif" width="160" height="140" style="opacity: 0; position: relative;"> -->
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var gif = document.getElementById('gif_image');
          var staticImage = document.getElementById('google');
          
          // Check if the GIF loads successfully
          gif.onerror = function () {
            gif.style.display = 'none';
            staticImage.style.display = 'block';
          };

          gif.onload = function () {
            gif.style.opacity = '1';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://link.springer.com/chapter/10.1007/978-3-031-27199-1_44">Gaze Detection Using Encoded Retinomorphic Events
</a>
      </papertitle>
      <br>
      Abeer Banerjee, Shyam Sunder Prasad, <strong>Naval Kishore Mehta</strong>, Himanshu Kumar, Sumeet Saurav, Sanjay Singh<br>
      <em>  International Conference on Intelligent Human Computer Interaction (IHCI), 2022</em><br>  
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-27199-1_44">Paper</a>
      <p></p>
      <p>
        We propose a novel six-channel event encoding technique to process saccadic motion event logs into structured images and design a Convolutional Neural Network for gaze prediction based on these encoded events. The approach is validated using metrics such as average distance, average angle, and pixel radius accuracy, demonstrating its reliability.   </p>
    </td>
  </tr>
</table>



<table>
  <tr bgcolor="#f5f5dc">
    <!-- Right: GIF/Image -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="image-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <img id="google" src="./Files/indicon22.png" width="160" height="140" style="position: absolute;">
        <!-- <img id="gif_image" src="./Files/animated_image.gif" width="160" height="140" style="opacity: 0; position: relative;"> -->
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var gif = document.getElementById('gif_image');
          var staticImage = document.getElementById('google');
          
          // Check if the GIF loads successfully
          gif.onerror = function () {
            gif.style.display = 'none';
            staticImage.style.display = 'block';
          };

          gif.onload = function () {
            gif.style.opacity = '1';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>

<!-- Left: Project Description -->
<td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
  <papertitle>
    <a href="https://ieeexplore.ieee.org/abstract/document/10039894">
      Real-Time Privacy-Preserving Fall Detection using Dynamic Vision Sensors
    </a>
  </papertitle>
  <br>
  Shyam Sunder Prasad, <strong>Naval Kishore Mehta</strong>, Himanshu Kumar, Abeer Banerjee, Sumeet Saurav, Sanjay Singh<br>
  <em>IEEE India Conference (INDICON), 2022</em><br>  
  <a href="https://ieeexplore.ieee.org/abstract/document/10039894" target="_blank">[Paper]</a> &nbsp; 
  <a href="https://docs.google.com/presentation/d/1ojcNfnqliQ_hFkOmvkxPbie23hePObbyLVyDDiZEt_4/edit?usp=sharing" target="_blank">[Slides]</a>
  <p></p>
  <p>
    This paper addresses the challenge of privacy-preserving fall detection, a subset of human action recognition, using the Dynamic Vision Sensor (DVS). We demonstrate real-time fall detection with a 3D-Convolutional Neural Network (3D-CNN), achieving an average sensitivity of 99.34% and specificity of 100%. With its small memory footprint, low parameter count, and efficient operation, the model delivers real-time performance on edge devices, making it practical for deployment.
  </p>
</td>

  </tr>
</table>



<table>
  <tr bgcolor="#f5f5dc">
    <!-- Right: GIF/Image -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="image-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <img id="google" src="./Files/spoof_paper.png" width="160" height="140" style="position: absolute;">
        <!-- <img id="gif_image" src="./Files/animated_image.gif" width="160" height="140" style="opacity: 0; position: relative;"> -->
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var gif = document.getElementById('gif_image');
          var staticImage = document.getElementById('google');
          
          // Check if the GIF loads successfully
          gif.onerror = function () {
            gif.style.display = 'none';
            staticImage.style.display = 'block';
          };

          gif.onload = function () {
            gif.style.opacity = '1';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://link.springer.com/chapter/10.1007/978-981-19-7513-4_10">A Self-explainable Face Anti-spoofing Solution Based on Depth Estimation
</a>
      </papertitle>
      <br>
      Shyam Sunder Prasad, <strong>Naval Kishore Mehta</strong>, Ankit Shukla, Pranav Mahajan, Arshdeep Singh, Sumeet Saurav, Sanjay Singh<br>
      <em> International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA), 2022</em><br>  
      <a href="https://link.springer.com/chapter/10.1007/978-981-19-7513-4_10">Paper</a> (Best Paper Award)
      <p></p>
      <p>
        We propose an explainable AI model that not only classifies spoof and live faces but also explains why they are different using mid-level features. Our proposed end-to-end face anti-spoofing network extracts the depth map and classifies the face input.      </p>
    </td>
  </tr>
</table>


<table>
  <tr bgcolor="#e6f7ff">
    <!-- Right: GIF/Image -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="image-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <img id="google" src="./Files/fras.png" width="180" height="140" style="position: absolute;">
        <!-- <img id="gif_image" src="./Files/animated_image.gif" width="160" height="140" style="opacity: 0; position: relative;"> -->
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var gif = document.getElementById('gif_image');
          var staticImage = document.getElementById('google');
          
          // Check if the GIF loads successfully
          gif.onerror = function () {
            gif.style.display = 'none';
            staticImage.style.display = 'block';
          };

          gif.onload = function () {
            gif.style.opacity = '1';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://csirnews.niscpr.res.in/home/article/163">Face Recognition based Attendance System (FRAS)</a>
      </papertitle>
      <br>
      Shyam Sunder Prasad, Prashant S Gidde, <strong>Naval Kishore Mehta</strong>, Arvind, Anil Saini, Bijender Kumar, Sumeet Saurav, Ravi Saini, Sanjay Singh, P. C. Panchariya<br>
      <em>CSIR-CEERI Foundation Day Technology Excellence Award, 2021</em><br>
      <em>All India 3rd Rank at       <a href="https://pib.gov.in/PressReleaseIframePage.aspx?PRID=1781091">India International Science Festival (IISF), 2021</a> </em><br>
      <em>Patent Applied</em>
      <p></p>
      <p>
        This project introduces an advanced attendance management system designed for versatility, scalability, and security. Capable of operating in both network-based and standalone configurations, it provides audio and video feedback for an enhanced user experience while maintaining a compact, memory-efficient database for long-term data retention. Powered by an AI model specifically trained for Indian faces, it incorporates anti-spoofing technology to ensure high security and accuracy. The system efficiently handles over 1500 users simultaneously and has been successfully deployed in more than 20 institutes, marking attendance within seconds with a flawless accuracy record. Built with in-house designed software and hardware, this innovative solution streamlines the attendance process with optimal functionality and reliability.
      </p>
    </td>
  </tr>
</table>


<!-- <details> -->

<!-- <summary> <font color="#0000ff"> More</font> </summary> -->



<table>
  <tr bgcolor="#e6f7ff">
    <!-- Right: GIF/Image -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="image-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <img id="google" src="./Files/shelps.png" width="180" height="140" style="position: absolute;">
        <!-- <img id="gif_image" src="./Files/animated_image.gif" width="160" height="140" style="opacity: 0; position: relative;"> -->
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var gif = document.getElementById('gif_image');
          var staticImage = document.getElementById('google');
          
          // Check if the GIF loads successfully
          gif.onerror = function () {
            gif.style.display = 'none';
            staticImage.style.display = 'block';
          };

          gif.onload = function () {
            gif.style.opacity = '1';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://youtu.be/IdHAy-6AFFU ">Student Engagement Level Prediction System (SELPS)</a>

      </papertitle>
      <br>
      <strong>Naval Kishore Mehta</strong>, Chander Mohan, Shyam S Prasad, Sanjay Singh<br>
      <em>2nd Prize in <a href="http://kutic.rusakuk.in/innovision-2019.html">INNOVISION-2019</a> </em><br>

      <p></p>
      <p>
       The system offers automatic and timely feedback to instructors, enhancing students' learning experiences by identifying absent-mindedness during lectures, improving content delivery effectiveness, and boosting learning outcomes. Key features include one-time face registration, engagement level prediction for each student, automated attendance tracking, and the ability to highlight students needing remedial classes based on individual scores. Also, it plots overall class engagement over time, offering actionable insights to optimize teaching strategies.
      </p>
    </td>
  </tr>
</table>


<table>
  <tr bgcolor="#f5f5dc">
    <!-- Right: Video -->
    <td style="padding:10px; width:25%; vertical-align:middle;">
      <div class="video-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <!-- WebM Video -->
        <video id="video_demo" width="180" height="140" autoplay muted loop controls>
          <source src="./Files/rps_demo.webm" type="video/webm">
          Your browser does not support the video tag.
        </video>
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var video = document.getElementById('video_demo');

          // Log video load status
          video.onerror = function () {
            console.log("Video failed to load.");
          };

          video.onloadstart = function () {
            console.log("Video loading...");
          };
        };
      </script>
    </td>
    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>

        <a href="https://www.youtube.com/watch?v=4OZy6LUb-ns">Real-Time Rock-Paper-Scissors Using CNN and HSV Camera</a>
      </papertitle>
      <br>  
      <strong>Naval Kishore Mehta</strong>,  <a href="https://www.sohandudala.in/">Sohan Dudala</a>, <a href="https://mmne.bits-hyderabad.ac.in/sangam-srikanth">S Srikanth</a>, Avinash, <a href="https://seeds.office.hiroshima-u.ac.jp/profile/en.73e73130814aed10520e17560c007669.html">Sushil Raut</a>, <a href="https://seeds.office.hiroshima-u.ac.jp/profile/en.47e0dee20f444458520e17560c007669.html">Idaku Ishii</a> <br>
      <em> <a href="https://ssp.jst.go.jp/en/">Sakura Science Exchange Program Fellow, 2019</a> </em><br>
      <em> <a href="http://www.ildp.hiroshima-u.ac.jp/">International Linkage Degree Program Fellowship (Hiroshima University, Japan), 2019</a> </em><br>
      <p></p>
      <p>
        This project implements a Convolutional Neural Network (CNN) to enable a robotic arm to play the classic game of Rock-Paper-Scissors. Leveraging an HSV camera, the system processes hand gestures in real-time and is deployed on an edge computing platform for optimal performance. The recognized gestures control the robotic arm, making this system efficient and responsive for human-robot interaction. 
              </p>
    </td>
  </tr>
</table>


<table>
  <tr bgcolor="#e6f7ff">
    <!-- Right: GIF/Image -->
    <td style="padding:10px;width:25%;vertical-align:middle">
      <div class="image-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <img id="google" src="./Files/thin_film.png" width="180" height="140" style="position: absolute;">
        <!-- <img id="gif_image" src="./Files/animated_image.gif" width="160" height="140" style="opacity: 0; position: relative;"> -->
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var gif = document.getElementById('gif_image');
          var staticImage = document.getElementById('google');
          
          // Check if the GIF loads successfully
          gif.onerror = function () {
            gif.style.display = 'none';
            staticImage.style.display = 'block';
          };

          gif.onload = function () {
            gif.style.opacity = '1';
            staticImage.style.display = 'none';
          };
        };
      </script>
    </td>
    </td>
    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>

      Preparation and characterization of Mn-Zn ferrite bulk and Mn-ferrite thin films
      </papertitle>

        <a href="./Files/IIRC posterpptx-1.pdf" target="_blank">[Poster]</a>
      <br>  
      <strong>Naval Kishore Mehta</strong> 

      <em> IIT Bomabay, 2017</a> </em><br>
      <p></p>
      <p>
        This project focuses on the deposition of Mn-ferrite thin films on Pt/Ti/SiOâ‚‚/Si substrates using Pulsed Laser Deposition (PLD), complemented by bulk material preparation. The structural and crystalline properties are characterized using X-ray Diffraction (XRD), ensuring phase purity and high-quality deposition. Mn-ferrite's versatile properties enable applications in magnetic sensors, spintronics, energy storage, RF/microwave devices, and environmental catalysis, making it a promising material for advanced technological innovations.
              </p>
    </td>
  </tr>
</table>




<table>
  <tr bgcolor="#f5f5dc">
    <!-- Right: Video -->
    <td style="padding:10px; width:25%; vertical-align:middle;">
      <div class="video-container" style="display: flex; align-items: center; justify-content: center; height: 100%;">
        <!-- WebM Video -->
        <video id="video_demo" width="180" height="140" autoplay muted loop controls>
          <source src="./Files/exoarm.webm" type="video/webm">
          Your browser does not support the video tag.
        </video>
      </div>
      <script type="text/javascript">
        window.onload = function () {
          var video = document.getElementById('video_demo');

          // Log video load status
          video.onerror = function () {
            console.log("Video failed to load.");
          };

          video.onloadstart = function () {
            console.log("Video loading...");
          };
        };
      </script>
    </td>

    <!-- Left: Project Description -->
    <td style="padding:20px;width:75%;vertical-align:middle; text-align: justify;">
      <papertitle>
        <a href="https://www.instructables.com/Exoskeleton-Arm/">ExoArm</a>
      </papertitle>
      <br>
      <strong>Naval Kishore Mehta</strong>, <a href="https://jila.colorado.edu/perkins/people/deopa">Surya Pratap Deopa</a>, Abhishek Saxena, <a href="https://www.linkedin.com/in/amit-jain-a5734745?originalSubdomain=in">Amit Jain</a> <br>
             <em>  <a href="https://www.analog.com/en/lp/002/india-anveshan.html">Anveshan Fellowship, 2014</a> </em><br>

      <p></p>
      <p>
        Exoskeleton Arm  is an active upper limb assistive device that is actuated by a High Torque servo Motor(Ï„ =  17.65197 Nm) and powered by two 11.2 V Li-Po battery connected in series .The whole system is commanded by bio-electric signals which pass by the brain to contract the target muscles. This can be used as physiotherapy device in areas of rehabilitation .Another use of this Exoskeleton Arm is in areas where heavy lifting is required. These areas may include heavy lifting in warehouses, factories and even in rescue operations.      </p>
    </td>
  </tr>
</table>


<!-- Volunteer Service -->
<A NAME="Services"><h2>Volunteer Services</h2></A>
<font size="3"> 
  <strong>Invited External Reviewing</strong>
  <ul>
    <li><strong>Journals:</strong></li>
    <ul>
      <li>Knowledge-Based Systems (KBS)</li>
      <li>Neural Networks</li>
      <li>Engineering Applications of Artificial Intelligence (EAAI)</li>
      <li>IEEE Transactions on Industrial Informatics</li>
       <li>The Journal of Supercomputing</li>
       
         <li>International Journal of Machine Learning and Cybernetics</li> 
       <li>Scientific Reports</li>
    </ul>
    <li><strong>Conferences:</strong></li>
    <ul>
      <li>Conference on Human Factors in Computing Systems (CHI)</li>
      <li>International Conference on Human-Robot Interaction (HRI)</li>
      <li>International Conference on Pattern Recognition (ICPR)</li>
      <li> International Joint Conference on Neural Networks (IJCNN) </li>
      <li>Annual Conference of the IEEE Industrial Electronics Society (IECON)</li>
    </ul>
  </ul>
</font>

  

  <strong>Professional Membership</strong>
  <ul>
    <li><strong>IEEE:</strong> Institute of Electrical and Electronics Engineers</li>
    <li><strong>IES:</strong> Industrial Electronics Society</li>
    <li><strong>ACM:</strong> Association for Computing Machinery </li>

  </ul>
<!-- </font> -->


 
<!-- <!-- Awards -->
<!-- <A NAME="Awards"><h2>Awards</h2></A>
<!-- <font size="3"> 
<!-- <ul> -->
<!--   <li> <strong>ACM SIGAI Travel Grant</strong>, 2024.</li>  
<!--   <li><strong>Senior Research Fellowship</strong>, CSIR India, 2024.</li>   -->
<!--   <li><strong>HU Global Pitch Challenge 2022</strong>, Hiroshima University (Online Mode), 2022.</li> -->
<!--   <li><strong>Best Research Paper Award</strong>, 10th International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA), 2022.</li> -->
<!--   <li>Padma Bhushan Dr. Amarjit Singh Memorial Foundation Day Award for <strong>Recognition of Excellence in Best Technological Solution</strong>, Team Member, CSIR-CEERI, 2021.</li> -->
<!--   <li>Secured <strong>2nd Position</strong> in INNOVISION-2019 organized by Kurukshetra University, India, 2020.</li> -->
<!--   <li>Awarded with <strong>Sakura Science Program</strong> and <strong>International Linkage Degree Program</strong> Fellowship at Hiroshima University, Japan, 2019.</li>
<!--   <li>Awarded with <strong>Gold Medal in M.Sc. Electronic Science</strong>, Kurukshetra University, India, by the Honorable Vice President of India, Muppavarapu Venkaiah Naidu, 2018.</li> -->
<!--   <li>Qualified <strong>Graduate Aptitude Test in Engineering (GATE)</strong> in Electronics and Communication conducted by IIT Guwahati, 2018.</li> -->
<!--   <li>Qualified <strong>UGC-National Eligibility Test</strong> in Electronic Science conducted by National Testing Agency (NTA), 2018.</li> -->
<!--   <li>Awarded with a prestigious six-month <strong>Research Internship</strong> by IIT Bombay, 2017.</li> -->  
<!--   <li><strong> Finalist in ANVESHAN-2014</strong>, a national biannual design fellowship program by Analog Devices India, 2015.</li> --> 

<!-- </ul> -->
<!-- </font> -->  

 

<!-- <br /> -->
<!-- <br /> -->
 

<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=a5VhcAJgSlGGHYoiYNA8XCV6WWD6RSpl8iexSkbr2TA&cl=ffffff&w=a"></script> -->
<!--
All Rights Reserved by Naval. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
-->

<!--
<font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2020.06.10</p>
</font>
-->

</body>
</html>
